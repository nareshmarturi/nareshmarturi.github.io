<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>

<head>
    <title>Naresh Marturi - Research</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="icon" href="images/lc.png" type="image/x-icon">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.min.css" />
    <link rel="stylesheet" href="assets/academicons-1.9.0/css/academicons.min.css" />
    <link rel="stylesheet" href="assets/css/main.css" />
    <link rel="stylesheet" href="assets/css/news.css" />
    <link rel="stylesheet" href="assets/css/divider.css">
</head>

<body>

    <!-- Nav -->
    <div class="nav">
        <nav id="nav" role="custom-dropdown">
            <input type="checkbox" id="button">
            <label for="button" onclick></label>
            <ul class="container">
                <li><a href="index.html">Home</a></li>
                <li><a href="about.html">About</a></li>
                <li><a href="#research">Research</a></li>
                <li><a href="publications.html">Publications</a></li>
                <li><a href="contact.html">Contact</a></li>
            </ul>
        </nav>
    </div>

    <a href="#" id="back-to-top" class="back-to-top" style="display: inline;">Top <i
            class="fas fa-chevron-circle-up"></i></a>
    <article id="research" class=" wrapper style1 animated fadeIn">
        <div class="container">
            <header>
                <h3 class="aboutblock-secheading">Selected Research</h3>
            </header>
            <div class="row aln-center">
                <div class="col-sm">
                    <section class="box style1">
                        <a href="#grasping"><span class="faicon fas fa-american-sign-language-interpreting"></span></a>
                        <h4><a href="#grasping">Robotic Grasping & Manipulation</a></h4>
                    </section>
                </div>
                <div class="col-sm">
                    <section class="box style1">
                        <a href="#compvis"><span class="faicon fas fa-eye"></span></a>
                        <h4><a href="#compvis">Computer Vision</a></h4>
                    </section>
                </div>
                <div class="col-sm">
                    <section class="box style1">
                        <a href="#visualservo"><span class="faicon fas fa-video"></span></a>
                        <h4><a href="#visualservo">Visual Servoing</a></h4>
                    </section>
                </div>
                <div class="col-sm">
                    <section class="box style1">
                        <a href="#teleman"><span class="faicon fas fa-gamepad"></span></a>
                        <h4><a href="#teleman">Tele-manipulation</a></h4>
                    </section>
                </div>
                <div class="col-sm">
                    <section class="box style1">
                        <a href="#nano"><span class="faicon fa fa-puzzle-piece"></span></a>
                        <h4><a href="#nano">Micro/Nano-manipulation</a></h4>
                    </section>
                </div>
            </div>
            <hr class="rounded">
            <div class="divider"><span></span><span></span><span></span></div>

            <!-- Grasping and Manipulation -->
            <div class="researchblock">
                <h2 id="grasping" class="aboutblock-secheading">Grasping and Manipulation</h2>

                <img src="images/g2.jpg" style="width:400px;">
                <div class="researchbloc-content">
                    <br>
                    <div class="aboutblock-secheading">
                        <h4>Grasping Moving Objects by Dynamic Re-planning: Human to Robot Handover</h4>
                    </div>
                    <p> This work shows how a robot arm can follow and grasp moving objects tracked by a vision system,
                        as is needed when a human hands over an object to the robot during collaborative working. While
                        the object is being arbitrarily moved by
                        the human co-worker, a set of likely grasps, generated by a learned grasp planner, are evaluated
                        online to generate a feasible grasp with respect to both: the current configuration of the robot
                        respecting the target grasp; and
                        the constraints of finding a collision-free trajectory to reach that configuration.
                        <!-- We propose a method of dynamic switching between: a local planner, where the hand smoothly tracks the object, maintaining a steady relative pre-grasp
                        pose; and a global planner, which rapidly moves the hand to a new grasp on a completely different part of the object, if the previous graspable part becomes unreachable. -->
                        <a href="https://link.springer.com/article/10.1007/s10514-018-9799-1" target="_blank"
                            class="boxed paper">Paper</a>
                        <span class="boxed nocode">Code</span> <a href="https://www.youtube.com/watch?v=kzG-TxT4wd8"
                            target="_blank" class="boxed video">Video</a>
                    </p>
                </div>
                <br>

                <img src="images/g1.png" style="width:550px;">
                <div class="researchbloc-content">
                    <div class="aboutblock-secheading">
                        <h4>Grasping with Local Contact Moments (LoCoMo)</h4>
                    </div>
                    <p> This work presents a new grasp planner to grasp arbitrarily shaped objects, observed as partial
                        point-clouds, without requiring: models of the objects, physics parameters, training data, or
                        other a-priori knowledge. A grasp metric
                        based on LoCoMo is proposed. <a href="https://ieeexplore.ieee.org/abstract/document/8594226"
                            target="_blank" class="boxed paper">Paper</a>
                        <a href="https://github.com/maximeadjigble/grasplocomo" target="_blank"
                            class="boxed code">Code</a><a href="https://www.youtube.com/watch?v=SEBD9T6Lvjs"
                            target="_blank" class="boxed video">Video</a>
                    </p>
                </div>
                <br>
                <img src="images/g3.png" style="width:550px;">
                <div class="researchbloc-content">
                    <div class="aboutblock-secheading">
                        <h4>Benchmarking protocol for grasp planning algorithms</h4>
                    </div>
                    <p> While acknowledging and discussing the outstanding difficulties surrounding the complex topic of
                        robotic grasping, we propose a methodology for reproducible experiments to compare the
                        performance of a variety of grasp planning algorithms.
                        Our protocol attempts to improve the objectivity with which different grasp planners are
                        compared. <a href="https://ieeexplore.ieee.org/abstract/document/8917672" target="_blank"
                            class="boxed paper">Paper</a>
                        <a href="https://github.com/maximeadjigble/grasplocomo" target="_blank"
                            class="boxed code">Code</a><a href="https://www.youtube.com/watch?v=SEBD9T6Lvjs"
                            target="_blank" class="boxed video">Video</a><a href="files/ynm_benchmark.pdf "
                            target="_blank" class="boxed file" download>Benchmark Template</a><a
                            href="files/ynm_protocol.pdf" target="_blank" class="boxed file">Protocol Template</a>
                    </p>
                </div>
                <br>
                <img src="images/g4.png" style="width:300px;">
                <div class="researchbloc-content">
                    <br>
                    <div class="aboutblock-secheading">
                        <h4>Grasping by tactile exploration</h4>
                    </div>
                    <p> This work addresses the problem of simultaneously exploring the shape of a partially unknown
                        object, using tactile sensors on robotic fingers, while also improving finger placement to
                        optimise grasp stability. We show how an initial
                        grasp attempt, based on an initial guess of the overall object shape, yields tactile glances of
                        the far side of the object which enable the shape estimate to be improved. We propose a
                        probabilistic representation of shape, and
                        shape uncertainty, based on Gaussian Process Implicit Surfaces. Combined with a probabilistic
                        estimate of grasp quality, we refine grasp configurations. <a
                            href="https://doi.org/10.1109/LRA.2021.3063074" target="_blank"
                            class="boxed paper">Paper</a>
                        <span class="boxed nocode">Code</span> <a href="https://youtu.be/w6k6SiVaKOQ" target="_blank"
                            class="boxed video">Video</a>
                    </p>
                </div>
                <br>
                <img src="images/spectgrasp.png" style="width:300px;">
                <div class="researchbloc-content">
                    <br>
                    <div class="aboutblock-secheading">
                        <h4>SpectGRASP: Robotic grasping by spectral correlation</h4>
                    </div>
                    <p> This work presents a spectral correlation-based method (SpectGRASP) for robotic grasping of
                        arbitrarily shaped, unknown objects. Given a point cloud of an object, SpectGRASP extracts
                        contact points on the object's surface matching
                        the hand configuration. It neither requires offline training nor a-priori object models. We
                        propose a novel Binary Extended Gaussian Image (BEGI), which represents the point cloud surface
                        normals of both object and robot fingers
                        as signals on a 2-sphere. Spherical harmonics are then used to estimate the correlation between
                        fingers and object BEGIs. <a href="https://arxiv.org/abs/2107.12492" target="_blank"
                            class="boxed paper">Paper</a>
                        <span class="boxed nocode">Code</span> <a href="https://youtu.be/Pev2fwVbJi0" target="_blank"
                            class="boxed video">Video</a>
                    </p>
                </div>
            </div>

            <hr class="rounded">
            <div class="divider"></div>

            <!-- Computer Vision -->
            <div class="researchblock">
                <h2 id="compvis">Computer Vision</h2>
                <img src="images/cv1.PNG" style="width:550px;">
                <div class="researchbloc-content">
                    <br>
                    <div class="aboutblock-secheading">
                        <h4>Human Action Recognition</h4>
                    </div>
                    <p> This work addresses both general and fine-grained human action recognition in video sequences.
                        Compared with general human actions, fine-grained action information is more difficult to detect
                        and occupies relatively small-scale image
                        regions. Our work improves fine-grained action discrimination, while also retaining the ability
                        to perform general action recognition. <a
                            href="https://www.sciencedirect.com/science/article/pii/S0031320317304788?" target="_blank"
                            class="boxed paper">Paper</a><span class="boxed nocode">Code</span>
                        <span class="boxed novideo">Video</span>
                    </p>
                </div>
                <br>
                <img src="images/cv2.PNG" style="width:550px;">
                <div class="researchbloc-content">
                    <br>
                    <div class="aboutblock-secheading">
                        <h4>Articulated (Human) Body Pose Tracking</h4>
                    </div>
                    <p> This work addresses the problem of online tracking of articulated human body poses in dynamic
                        environments. We propose a coupled-layer framework, which uses the previous notions of
                        deformable structure (DS) puppet models. The underlying
                        idea is to decompose the global pose candidate in any particular frame into several local parts
                        to obtain a refined pose. <a
                            href="https://www.sciencedirect.com/science/article/pii/S1077314216301187" target="_blank"
                            class="boxed paper">Paper</a> <span class="boxed nocode">Code</span> <a
                            href="https://youtu.be/isi-YoUF6WI" target="_blank" class="boxed video">Video</a></p>
                </div>
            </div>
            <hr class="rounded">
            <div class="divider"><span></span><span></span><span></span></div>

            <!-- Visual Servoing -->

            <div class="researchblock">
                <h2 id="visualservo">Vision-Guided Robot Control</h2>
                <img src="images/Fig1.png" style="width:300px;">
                <div class="researchbloc-content">
                    <br>
                    <div class="aboutblock-secheading">
                        <h4>Dual quaternion-based visual servoing for grasping moving objects</h4>
                    </div>
                    <p> This work presents a new dual quaternion-based formulation for pose-based visual servoing.
                        Extending our previous work on local contact moment (LoCoMo) based grasp planning, we
                        demonstrate grasping of arbitrarily moving objects in
                        3D space. Dual quaternions allow designing the visual servoing task in a more compact manner and
                        provide robustness to manipulator singularities. Given an object point cloud, LoCoMo generates a
                        ranked list of grasp and pre-grasp
                        poses, which are used as desired poses for visual servoing. Whenever the object moves (tracked
                        by visual marker tracking), the desired pose updates, the robot tracks and grasps movig objects.
                        <a href="https://arxiv.org/abs/2107.08149" target="_blank" class="boxed paper">Paper</a>
                        <span class="boxed nocode">Code</span> <span class="boxed novideo">Video</span>
                    </p>
                </div>
                <br>
                <img src="images/vs1.gif" style="width:350px;">
                <div class="researchbloc-content">
                    <div class="aboutblock-secheading">
                        <h4>Fourier-based visual servoing for nano positioning</h4>
                    </div>
                    <p> Mostly, the positioning tasks inside a scanning electron microscope (SEM) are exhibited by
                        skilled operators via teleoperation. Nevertheless, it is still a difficult task to repeat and
                        hence automatic strategies are indispensable.
                        The regular VS strategies are hard to use with SEM due to multiple instabilities associated with
                        the imaging process. Adressing this, we present an image frequency-based positioning stage
                        controller that does not require any visual
                        tracking and is capable of dealing with electronic images provided by a SEM for automatic
                        nanopositioning. <a href="https://ieeexplore.ieee.org/abstract/document/7506229" target="_blank"
                            class="boxed paper">Paper</a>
                        <span class="boxed nocode">Code</span> <a href="https://www.youtube.com/watch?v=LT7towm8EJQ"
                            target="_blank" class="boxed video">Video</a>
                    </p>
                </div>
                <br>
                <img src="images/vs3.png" style="width:300px;">
                <div class="researchbloc-content">
                    <br>
                    <div class="aboutblock-secheading">
                        <h4>Visual Servoing Scheme for Nuclear Waste Decommissioning</h4>
                    </div>
                    <p> Despite enormous remote handling requirements, there has been remarkably little use of robots by
                        the nuclear industry. The few robots deployed have been directly teleoperated in rudimentary
                        ways, with no advanced control methods or
                        autonomy. Most remote handling is still done by an aging workforce of highly skilled experts,
                        using 1960s style mechanical Master-Slave devices. In contrast, we explore how novice human
                        operators can rapidly learn to control modern
                        robots to perform basic manipulation tasks. <a
                            href="https://ieeexplore.ieee.org/document/7759525" target="_blank"
                            class="boxed paper">Paper</a>
                        <span class="boxed nocode">Code</span> <span class="boxed novideo">Video</span>
                    </p>
                </div>
                <br>

                <img src="images/vs2.png" style="width:300px;">
                <div class="researchbloc-content">
                    <br>
                    <div class="aboutblock-secheading">
                        <h4>Vision-guided Robot State Estimation</h4>
                    </div>
                    <p> This work presents a vision-based approach for estimating the configuration of, and providing
                        control signals for, an under-sensored robot manipulator using a single monocular camera. Some
                        remote manipulators, used for decommissioning
                        tasks in the nuclear industry, lack proprioceptive sensors because electronics are vulnerable to
                        radiation. In these scenarios, it would be beneficial to use external sensory information, e.g.
                        vision, for estimating the robot configuration
                        with respect to the scene or task. We propose two methods for this purpose. <a
                            href="https://ieeexplore.ieee.org/document/7759525" target="_blank"
                            class="boxed paper">Paper</a>
                        <span class="boxed nocode">Code</span> <a href="https://youtu.be/my_d5-2u78Y" target="_blank"
                            class="boxed video">Video</a>
                    </p>
                </div>
            </div>

            <hr class="rounded">
            <div class="divider"><span></span><span></span><span></span></div>

            <!-- Tele-manipulation -->

            <div class="researchblock">
                <h2 id="teleman">Robotic Tele-Manipulation</h2>

                <img src="images/te1.png" style="width:550px;">
                <div class="researchbloc-content">
                    <div class="aboutblock-secheading">
                        <h4>Assisted Telemanipulation: with Integrated Grasp Planning</h4>
                    </div>
                    <p> This work presents an assisted telemanipulation approach with integrated grasp planning. It also
                        studies how the human teleoperation performance benefits from the incorporated visual and haptic
                        cues while manipulating objects in cluttered
                        environments. The developed system combines the widely used master-slave teleoperation with our
                        previous modelfree and learning-free grasping algorithm by means of a dynamic grasp re-ranking
                        strategy and a semi-autonomous reach-to-grasp
                        trajectory guidance. <a href="https://ieeexplore.ieee.org/abstract/document/8968454"
                            target="_blank" class="boxed paper">Paper</a>
                        <span class="boxed nocode">Code</span> <a href="https://www.youtube.com/watch?v=ALGZ-y3Ui94"
                            target="_blank" class="boxed video">Video</a>
                    </p>
                </div>
                <br>
                <img src="images/te2.png" style="width:300px;">
                <div class="researchbloc-content">
                    <br>
                    <div class="aboutblock-secheading">
                        <h4>Singularity-Robust Inverse Kinematics Solver for Tele-manipulation</h4>
                    </div>
                    <p> This work investigates the effect of inverse kinematics (IK) on operator performance during the
                        telemanipulation of an industrial robot. Inspired by a successful algorithm used in computer
                        graphics to solve the IK problem and devise
                        smooth movements (FABRIK), we developed a new optimisation-based IK solver that is robust with
                        respect to the robot's singularities and assists the operator in generating smooth trajectories.
                        Our algorithm takes advantage of the
                        kinematic structure of the robot in order to decouple the notoriously difficult IK problem of
                        orientation and position. <a href="https://ieeexplore.ieee.org/document/8842871" target="_blank"
                            class="boxed paper">Paper</a>
                        <span class="boxed nocode">Code</span> <a href="https://www.youtube.com/watch?v=ImYof82m8fg"
                            target="_blank" class="boxed video">Video</a>
                    </p>
                </div>
            </div>

            <hr class="rounded">
            <div class="divider"></div>
            <!-- Micro-Nano Manipulation -->

            <div class="researchblock">
                <h2 id="nano">Micro-Nano Manipulation</h2>
                <img src="images/mn1.png" style="width:450px;">
                <div class="researchbloc-content">
                    <div class="aboutblock-secheading">
                        <h4>Visual Servoing-Based Autofocus for Scanning Electron Microscope</h4>
                    </div>
                    <p>
                        <a href="https://ieeexplore.ieee.org/abstract/document/6696734" target="_blank"
                            class="boxed paper">Paper</a>
                        <span class="boxed nocode">Code</span> <a href="https://youtu.be/n5TYW3oNcrk" target="_blank"
                            class="boxed video">Video</a>
                    </p>
                </div>
            </div>
        </div>
        <br><br>
        <!-- Site footer -->
        <footer>
            <ul id="copyright">
                <li>&copy 2022.</li>
                <li><a href="index.html#top">Dr Naresh Marturi</a></li>
                <li>All rights reserved.</a>
                </li>
            </ul>
        </footer>

    </article>
    <!-- Scripts -->
    <script src="assets/js/jquery.min.js "></script>
    <script src="assets/js/jquery.scrolly.min.js "></script>
    <script src="assets/js/browser.min.js "></script>
    <script src="assets/js/breakpoints.min.js "></script>
    <script src="assets/js/util.js "></script>
    <script src="assets/js/main.js "></script>
    <script src="assets/js/trans.js "></script>
    <script src="assets/js/main.js "></script>
    <script src="assets/js/scroller.js "></script>
</body>

</html>